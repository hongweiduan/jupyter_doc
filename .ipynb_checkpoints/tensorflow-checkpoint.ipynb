{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a simple e.g."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get some phony data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "x_data = np.float32(np.random.rand(2,100))\n",
    "y_data = np.dot([0.100,0.200],x_data) + 0.300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "b = tf.Variable(tf.zeros([1]))\n",
    "W = tf.Variable(tf.random_uniform([1,2],-1.0,10))\n",
    "y = tf.matmul(W,x_data) + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"add:0\", shape=(1, 100), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# help(tf.Variable)\n",
    "print y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class Variable in module tensorflow.python.ops.variables:\n",
      "\n",
      "class Variable(__builtin__.object)\n",
      " |  See the @{$variables$Variables How To} for a high\n",
      " |  level overview.\n",
      " |  \n",
      " |  A variable maintains state in the graph across calls to `run()`. You add a\n",
      " |  variable to the graph by constructing an instance of the class `Variable`.\n",
      " |  \n",
      " |  The `Variable()` constructor requires an initial value for the variable,\n",
      " |  which can be a `Tensor` of any type and shape. The initial value defines the\n",
      " |  type and shape of the variable. After construction, the type and shape of\n",
      " |  the variable are fixed. The value can be changed using one of the assign\n",
      " |  methods.\n",
      " |  \n",
      " |  If you want to change the shape of a variable later you have to use an\n",
      " |  `assign` Op with `validate_shape=False`.\n",
      " |  \n",
      " |  Just like any `Tensor`, variables created with `Variable()` can be used as\n",
      " |  inputs for other Ops in the graph. Additionally, all the operators\n",
      " |  overloaded for the `Tensor` class are carried over to variables, so you can\n",
      " |  also add nodes to the graph by just doing arithmetic on variables.\n",
      " |  \n",
      " |  ```python\n",
      " |  import tensorflow as tf\n",
      " |  \n",
      " |  # Create a variable.\n",
      " |  w = tf.Variable(<initial-value>, name=<optional-name>)\n",
      " |  \n",
      " |  # Use the variable in the graph like any Tensor.\n",
      " |  y = tf.matmul(w, ...another variable or tensor...)\n",
      " |  \n",
      " |  # The overloaded operators are available too.\n",
      " |  z = tf.sigmoid(w + y)\n",
      " |  \n",
      " |  # Assign a new value to the variable with `assign()` or a related method.\n",
      " |  w.assign(w + 1.0)\n",
      " |  w.assign_add(1.0)\n",
      " |  ```\n",
      " |  \n",
      " |  When you launch the graph, variables have to be explicitly initialized before\n",
      " |  you can run Ops that use their value. You can initialize a variable by\n",
      " |  running its *initializer op*, restoring the variable from a save file, or\n",
      " |  simply running an `assign` Op that assigns a value to the variable. In fact,\n",
      " |  the variable *initializer op* is just an `assign` Op that assigns the\n",
      " |  variable's initial value to the variable itself.\n",
      " |  \n",
      " |  ```python\n",
      " |  # Launch the graph in a session.\n",
      " |  with tf.Session() as sess:\n",
      " |      # Run the variable initializer.\n",
      " |      sess.run(w.initializer)\n",
      " |      # ...you now can run ops that use the value of 'w'...\n",
      " |  ```\n",
      " |  \n",
      " |  The most common initialization pattern is to use the convenience function\n",
      " |  `global_variables_initializer()` to add an Op to the graph that initializes\n",
      " |  all the variables. You then run that Op after launching the graph.\n",
      " |  \n",
      " |  ```python\n",
      " |  # Add an Op to initialize global variables.\n",
      " |  init_op = tf.global_variables_initializer()\n",
      " |  \n",
      " |  # Launch the graph in a session.\n",
      " |  with tf.Session() as sess:\n",
      " |      # Run the Op that initializes global variables.\n",
      " |      sess.run(init_op)\n",
      " |      # ...you can now run any Op that uses variable values...\n",
      " |  ```\n",
      " |  \n",
      " |  If you need to create a variable with an initial value dependent on another\n",
      " |  variable, use the other variable's `initialized_value()`. This ensures that\n",
      " |  variables are initialized in the right order.\n",
      " |  \n",
      " |  All variables are automatically collected in the graph where they are\n",
      " |  created. By default, the constructor adds the new variable to the graph\n",
      " |  collection `GraphKeys.GLOBAL_VARIABLES`. The convenience function\n",
      " |  `global_variables()` returns the contents of that collection.\n",
      " |  \n",
      " |  When building a machine learning model it is often convenient to distinguish\n",
      " |  between variables holding the trainable model parameters and other variables\n",
      " |  such as a `global step` variable used to count training steps. To make this\n",
      " |  easier, the variable constructor supports a `trainable=<bool>` parameter. If\n",
      " |  `True`, the new variable is also added to the graph collection\n",
      " |  `GraphKeys.TRAINABLE_VARIABLES`. The convenience function\n",
      " |  `trainable_variables()` returns the contents of this collection. The\n",
      " |  various `Optimizer` classes use this collection as the default list of\n",
      " |  variables to optimize.\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __abs__ = _run_op(a, *args)\n",
      " |      Computes the absolute value of a tensor.\n",
      " |      \n",
      " |      Given a tensor of real numbers `x`, this operation returns a tensor\n",
      " |      containing the absolute value of each element in `x`. For example, if x is\n",
      " |      an input element and y is an output element, this operation computes\n",
      " |      \\\\\\\\(y = |x|\\\\\\\\).\n",
      " |      \n",
      " |      Args:\n",
      " |        x: A `Tensor` or `SparseTensor` of type `float32`, `float64`, `int32`, or\n",
      " |          `int64`.\n",
      " |        name: A name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor` or `SparseTensor` the same size and type as `x` with absolute\n",
      " |          values.\n",
      " |  \n",
      " |  __add__ = _run_op(a, *args)\n",
      " |      Returns x + y element-wise.\n",
      " |      \n",
      " |      *NOTE*: `Add` supports broadcasting. `AddN` does not. More about broadcasting\n",
      " |      [here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)\n",
      " |      \n",
      " |      Args:\n",
      " |        x: A `Tensor`. Must be one of the following types: `half`, `float32`, `float64`, `uint8`, `int8`, `int16`, `int32`, `int64`, `complex64`, `complex128`, `string`.\n",
      " |        y: A `Tensor`. Must have the same type as `x`.\n",
      " |        name: A name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor`. Has the same type as `x`.\n",
      " |  \n",
      " |  __and__ = _run_op(a, *args)\n",
      " |      Returns the truth value of x AND y element-wise.\n",
      " |      \n",
      " |      *NOTE*: `LogicalAnd` supports broadcasting. More about broadcasting\n",
      " |      [here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)\n",
      " |      \n",
      " |      Args:\n",
      " |        x: A `Tensor` of type `bool`.\n",
      " |        y: A `Tensor` of type `bool`.\n",
      " |        name: A name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor` of type `bool`.\n",
      " |  \n",
      " |  __div__ = _run_op(a, *args)\n",
      " |      Divide two values using Python 2 semantics. Used for Tensor.__div__.\n",
      " |      \n",
      " |      Args:\n",
      " |        x: `Tensor` numerator of real numeric type.\n",
      " |        y: `Tensor` denominator of real numeric type.\n",
      " |        name: A name for the operation (optional).\n",
      " |      Returns:\n",
      " |        `x / y` returns the quotient of x and y.\n",
      " |  \n",
      " |  __floordiv__ = _run_op(a, *args)\n",
      " |      Divides `x / y` elementwise, rounding toward the most negative integer.\n",
      " |      \n",
      " |      The same as `tf.div(x,y)` for integers, but uses `tf.floor(tf.div(x,y))` for\n",
      " |      floating point arguments so that the result is always an integer (though\n",
      " |      possibly an integer represented as floating point).  This op is generated by\n",
      " |      `x // y` floor division in Python 3 and in Python 2.7 with\n",
      " |      `from __future__ import division`.\n",
      " |      \n",
      " |      Note that for efficiency, `floordiv` uses C semantics for negative numbers\n",
      " |      (unlike Python and Numpy).\n",
      " |      \n",
      " |      `x` and `y` must have the same type, and the result will have the same type\n",
      " |      as well.\n",
      " |      \n",
      " |      Args:\n",
      " |        x: `Tensor` numerator of real numeric type.\n",
      " |        y: `Tensor` denominator of real numeric type.\n",
      " |        name: A name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        `x / y` rounded down (except possibly towards zero for negative integers).\n",
      " |      \n",
      " |      Raises:\n",
      " |        TypeError: If the inputs are complex.\n",
      " |  \n",
      " |  __ge__ = _run_op(a, *args)\n",
      " |      Returns the truth value of (x >= y) element-wise.\n",
      " |      \n",
      " |      *NOTE*: `GreaterEqual` supports broadcasting. More about broadcasting\n",
      " |      [here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)\n",
      " |      \n",
      " |      Args:\n",
      " |        x: A `Tensor`. Must be one of the following types: `float32`, `float64`, `int32`, `int64`, `uint8`, `int16`, `int8`, `uint16`, `half`.\n",
      " |        y: A `Tensor`. Must have the same type as `x`.\n",
      " |        name: A name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor` of type `bool`.\n",
      " |  \n",
      " |  __getitem__ = _SliceHelperVar(var, slice_spec)\n",
      " |      Creates a slice helper object given a variable.\n",
      " |      \n",
      " |      This allows creating a sub-tensor from part of the current contents\n",
      " |      of a variable.  See ${tf.Tensor$`Tensor.__getitem__`}\n",
      " |      for detailed examples of slicing.\n",
      " |      \n",
      " |      This function in addition also allows assignment to a sliced range.\n",
      " |      This is similar to `__setitem__` functionality in Python. However,\n",
      " |      the syntax is different so that the user can capture the assignment\n",
      " |      operation for grouping or passing to `sess.run()`.\n",
      " |      For example,\n",
      " |      \n",
      " |      ```prettyprint\n",
      " |      import tensorflow as tf\n",
      " |      A = tf.Variable([[1,2,3], [4,5,6], [7,8,9]], dtype=tf.float32)\n",
      " |      with tf.Session() as sess:\n",
      " |        sess.run(tf.global_variables_initializer())\n",
      " |        print sess.run(A[:2, :2]) # => [[1,2], [4,5]]\n",
      " |      \n",
      " |        op = A[:2,:2].assign(22. * tf.ones((2, 2)))\n",
      " |        print sess.run(op) # => [[22, 22, 3], [22, 22, 6], [7,8,9]]\n",
      " |      ```\n",
      " |      \n",
      " |      Note that assignments currently do not support NumPy broadcasting\n",
      " |      semantics.\n",
      " |      \n",
      " |      Args:\n",
      " |        var: An `ops.Variable` object.\n",
      " |        slice_spec: The arguments to `Tensor.__getitem__`.\n",
      " |      \n",
      " |      Returns:\n",
      " |        The appropriate slice of \"tensor\", based on \"slice_spec\".\n",
      " |        As an operator. The operator also has a `assign()` method\n",
      " |        that can be used to generate an assignment operator.\n",
      " |      \n",
      " |      Raises:\n",
      " |        ValueError: If a slice range is negative size.\n",
      " |        TypeError: If the slice indices aren't int, slice, or Ellipsis.\n",
      " |  \n",
      " |  __gt__ = _run_op(a, *args)\n",
      " |      Returns the truth value of (x > y) element-wise.\n",
      " |      \n",
      " |      *NOTE*: `Greater` supports broadcasting. More about broadcasting\n",
      " |      [here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)\n",
      " |      \n",
      " |      Args:\n",
      " |        x: A `Tensor`. Must be one of the following types: `float32`, `float64`, `int32`, `int64`, `uint8`, `int16`, `int8`, `uint16`, `half`.\n",
      " |        y: A `Tensor`. Must have the same type as `x`.\n",
      " |        name: A name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor` of type `bool`.\n",
      " |  \n",
      " |  __init__(self, initial_value=None, trainable=True, collections=None, validate_shape=True, caching_device=None, name=None, variable_def=None, dtype=None, expected_shape=None, import_scope=None)\n",
      " |      Creates a new variable with value `initial_value`.\n",
      " |      \n",
      " |      The new variable is added to the graph collections listed in `collections`,\n",
      " |      which defaults to `[GraphKeys.GLOBAL_VARIABLES]`.\n",
      " |      \n",
      " |      If `trainable` is `True` the variable is also added to the graph collection\n",
      " |      `GraphKeys.TRAINABLE_VARIABLES`.\n",
      " |      \n",
      " |      This constructor creates both a `variable` Op and an `assign` Op to set the\n",
      " |      variable to its initial value.\n",
      " |      \n",
      " |      Args:\n",
      " |        initial_value: A `Tensor`, or Python object convertible to a `Tensor`,\n",
      " |          which is the initial value for the Variable. The initial value must have\n",
      " |          a shape specified unless `validate_shape` is set to False. Can also be a\n",
      " |          callable with no argument that returns the initial value when called. In\n",
      " |          that case, `dtype` must be specified. (Note that initializer functions\n",
      " |          from init_ops.py must first be bound to a shape before being used here.)\n",
      " |        trainable: If `True`, the default, also adds the variable to the graph\n",
      " |          collection `GraphKeys.TRAINABLE_VARIABLES`. This collection is used as\n",
      " |          the default list of variables to use by the `Optimizer` classes.\n",
      " |        collections: List of graph collections keys. The new variable is added to\n",
      " |          these collections. Defaults to `[GraphKeys.GLOBAL_VARIABLES]`.\n",
      " |        validate_shape: If `False`, allows the variable to be initialized with a\n",
      " |          value of unknown shape. If `True`, the default, the shape of\n",
      " |          `initial_value` must be known.\n",
      " |        caching_device: Optional device string describing where the Variable\n",
      " |          should be cached for reading.  Defaults to the Variable's device.\n",
      " |          If not `None`, caches on another device.  Typical use is to cache\n",
      " |          on the device where the Ops using the Variable reside, to deduplicate\n",
      " |          copying through `Switch` and other conditional statements.\n",
      " |        name: Optional name for the variable. Defaults to `'Variable'` and gets\n",
      " |          uniquified automatically.\n",
      " |        variable_def: `VariableDef` protocol buffer. If not `None`, recreates\n",
      " |          the Variable object with its contents, referencing the variable's nodes\n",
      " |          in the graph, which must already exist. The graph is not changed.\n",
      " |          `variable_def` and the other arguments are mutually exclusive.\n",
      " |        dtype: If set, initial_value will be converted to the given type.\n",
      " |          If `None`, either the datatype will be kept (if `initial_value` is\n",
      " |          a Tensor), or `convert_to_tensor` will decide.\n",
      " |        expected_shape: A TensorShape. If set, initial_value is expected\n",
      " |          to have this shape.\n",
      " |        import_scope: Optional `string`. Name scope to add to the\n",
      " |          `Variable.` Only used when initializing from protocol buffer.\n",
      " |      \n",
      " |      Raises:\n",
      " |        ValueError: If both `variable_def` and initial_value are specified.\n",
      " |        ValueError: If the initial value is not specified, or does not have a\n",
      " |          shape and `validate_shape` is `True`.\n",
      " |  \n",
      " |  __invert__ = _run_op(a, *args)\n",
      " |      Returns the truth value of NOT x element-wise.\n",
      " |      \n",
      " |      Args:\n",
      " |        x: A `Tensor` of type `bool`.\n",
      " |        name: A name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor` of type `bool`.\n",
      " |  \n",
      " |  __iter__(self)\n",
      " |      Dummy method to prevent iteration. Do not call.\n",
      " |      \n",
      " |      NOTE(mrry): If we register __getitem__ as an overloaded operator,\n",
      " |      Python will valiantly attempt to iterate over the variable's Tensor from 0\n",
      " |      to infinity.  Declaring this method prevents this unintended behavior.\n",
      " |      \n",
      " |      Raises:\n",
      " |        TypeError: when invoked.\n",
      " |  \n",
      " |  __le__ = _run_op(a, *args)\n",
      " |      Returns the truth value of (x <= y) element-wise.\n",
      " |      \n",
      " |      *NOTE*: `LessEqual` supports broadcasting. More about broadcasting\n",
      " |      [here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)\n",
      " |      \n",
      " |      Args:\n",
      " |        x: A `Tensor`. Must be one of the following types: `float32`, `float64`, `int32`, `int64`, `uint8`, `int16`, `int8`, `uint16`, `half`.\n",
      " |        y: A `Tensor`. Must have the same type as `x`.\n",
      " |        name: A name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor` of type `bool`.\n",
      " |  \n",
      " |  __lt__ = _run_op(a, *args)\n",
      " |      Returns the truth value of (x < y) element-wise.\n",
      " |      \n",
      " |      *NOTE*: `Less` supports broadcasting. More about broadcasting\n",
      " |      [here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)\n",
      " |      \n",
      " |      Args:\n",
      " |        x: A `Tensor`. Must be one of the following types: `float32`, `float64`, `int32`, `int64`, `uint8`, `int16`, `int8`, `uint16`, `half`.\n",
      " |        y: A `Tensor`. Must have the same type as `x`.\n",
      " |        name: A name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor` of type `bool`.\n",
      " |  \n",
      " |  __matmul__ = _run_op(a, *args)\n",
      " |      Multiplies matrix `a` by matrix `b`, producing `a` * `b`.\n",
      " |      \n",
      " |      The inputs must be matrices (or tensors of rank > 2, representing batches of\n",
      " |      matrices), with matching inner dimensions, possibly after transposition.\n",
      " |      \n",
      " |      Both matrices must be of the same type. The supported types are:\n",
      " |      `float16`, `float32`, `float64`, `int32`, `complex64`, `complex128`.\n",
      " |      \n",
      " |      Either matrix can be transposed or adjointed (conjugated and transposed) on\n",
      " |      the fly by setting one of the corresponding flag to `True`. These are `False`\n",
      " |      by default.\n",
      " |      \n",
      " |      If one or both of the matrices contain a lot of zeros, a more efficient\n",
      " |      multiplication algorithm can be used by setting the corresponding\n",
      " |      `a_is_sparse` or `b_is_sparse` flag to `True`. These are `False` by default.\n",
      " |      This optimization is only available for plain matrices (rank-2 tensors) with\n",
      " |      datatypes `bfloat16` or `float32`.\n",
      " |      \n",
      " |      For example:\n",
      " |      \n",
      " |      ```python\n",
      " |      # 2-D tensor `a`\n",
      " |      a = tf.constant([1, 2, 3, 4, 5, 6], shape=[2, 3]) => [[1. 2. 3.]\n",
      " |                                                            [4. 5. 6.]]\n",
      " |      # 2-D tensor `b`\n",
      " |      b = tf.constant([7, 8, 9, 10, 11, 12], shape=[3, 2]) => [[7. 8.]\n",
      " |                                                               [9. 10.]\n",
      " |                                                               [11. 12.]]\n",
      " |      c = tf.matmul(a, b) => [[58 64]\n",
      " |                              [139 154]]\n",
      " |      \n",
      " |      \n",
      " |      # 3-D tensor `a`\n",
      " |      a = tf.constant(np.arange(1, 13, dtype=np.int32),\n",
      " |                      shape=[2, 2, 3])                  => [[[ 1.  2.  3.]\n",
      " |                                                             [ 4.  5.  6.]],\n",
      " |                                                            [[ 7.  8.  9.]\n",
      " |                                                             [10. 11. 12.]]]\n",
      " |      \n",
      " |      # 3-D tensor `b`\n",
      " |      b = tf.constant(np.arange(13, 25, dtype=np.int32),\n",
      " |                      shape=[2, 3, 2])                   => [[[13. 14.]\n",
      " |                                                              [15. 16.]\n",
      " |                                                              [17. 18.]],\n",
      " |                                                             [[19. 20.]\n",
      " |                                                              [21. 22.]\n",
      " |                                                              [23. 24.]]]\n",
      " |      c = tf.matmul(a, b) => [[[ 94 100]\n",
      " |                               [229 244]],\n",
      " |                              [[508 532]\n",
      " |                               [697 730]]]\n",
      " |      \n",
      " |      # Since python >= 3.5 the @ operator is supported (see PEP 465).\n",
      " |      # In TensorFlow, it simply calls the `tf.matmul()` function, so the\n",
      " |      # following lines are equivalent:\n",
      " |      d = a @ b @ [[10.], [11.]]\n",
      " |      d = tf.matmul(tf.matmul(a, b), [[10.], [11.]])\n",
      " |      ```\n",
      " |      \n",
      " |      Args:\n",
      " |        a: `Tensor` of type `float16`, `float32`, `float64`, `int32`, `complex64`,\n",
      " |          `complex128` and rank > 1.\n",
      " |        b: `Tensor` with same type and rank as `a`.\n",
      " |        transpose_a: If `True`, `a` is transposed before multiplication.\n",
      " |        transpose_b: If `True`, `b` is transposed before multiplication.\n",
      " |        adjoint_a: If `True`, `a` is conjugated and transposed before\n",
      " |          multiplication.\n",
      " |        adjoint_b: If `True`, `b` is conjugated and transposed before\n",
      " |          multiplication.\n",
      " |        a_is_sparse: If `True`, `a` is treated as a sparse matrix.\n",
      " |        b_is_sparse: If `True`, `b` is treated as a sparse matrix.\n",
      " |        name: Name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor` of the same type as `a` and `b` where each inner-most matrix is\n",
      " |        the product of the corresponding matrices in `a` and `b`, e.g. if all\n",
      " |        transpose or adjoint attributes are `False`:\n",
      " |      \n",
      " |        `output`[..., i, j] = sum_k (`a`[..., i, k] * `b`[..., k, j]),\n",
      " |        for all indices i, j.\n",
      " |      \n",
      " |        Note: This is matrix product, not element-wise product.\n",
      " |      \n",
      " |      \n",
      " |      Raises:\n",
      " |        ValueError: If transpose_a and adjoint_a, or transpose_b and adjoint_b\n",
      " |          are both set to True.\n",
      " |  \n",
      " |  __mod__ = _run_op(a, *args)\n",
      " |      Returns element-wise remainder of division. When `x < 0` xor `y < 0` is\n",
      " |      \n",
      " |      true, this follows Python semantics in that the result here is consistent\n",
      " |      with a flooring divide. E.g. `floor(x / y) * y + mod(x, y) = x`.\n",
      " |      \n",
      " |      *NOTE*: `FloorMod` supports broadcasting. More about broadcasting\n",
      " |      [here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)\n",
      " |      \n",
      " |      Args:\n",
      " |        x: A `Tensor`. Must be one of the following types: `int32`, `int64`, `float32`, `float64`.\n",
      " |        y: A `Tensor`. Must have the same type as `x`.\n",
      " |        name: A name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor`. Has the same type as `x`.\n",
      " |  \n",
      " |  __mul__ = _run_op(a, *args)\n",
      " |      Dispatches cwise mul for \"Dense*Dense\" and \"Dense*Sparse\".\n",
      " |  \n",
      " |  __neg__ = _run_op(a, *args)\n",
      " |      Computes numerical negative value element-wise.\n",
      " |      \n",
      " |      I.e., \\\\(y = -x\\\\).\n",
      " |      \n",
      " |      Args:\n",
      " |        x: A `Tensor`. Must be one of the following types: `half`, `float32`, `float64`, `int32`, `int64`, `complex64`, `complex128`.\n",
      " |        name: A name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor`. Has the same type as `x`.\n",
      " |  \n",
      " |  __or__ = _run_op(a, *args)\n",
      " |      Returns the truth value of x OR y element-wise.\n",
      " |      \n",
      " |      *NOTE*: `LogicalOr` supports broadcasting. More about broadcasting\n",
      " |      [here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)\n",
      " |      \n",
      " |      Args:\n",
      " |        x: A `Tensor` of type `bool`.\n",
      " |        y: A `Tensor` of type `bool`.\n",
      " |        name: A name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor` of type `bool`.\n",
      " |  \n",
      " |  __pow__ = _run_op(a, *args)\n",
      " |      Computes the power of one value to another.\n",
      " |      \n",
      " |      Given a tensor `x` and a tensor `y`, this operation computes \\\\\\\\(x^y\\\\\\\\) for\n",
      " |      corresponding elements in `x` and `y`. For example:\n",
      " |      \n",
      " |      ```\n",
      " |      # tensor 'x' is [[2, 2], [3, 3]]\n",
      " |      # tensor 'y' is [[8, 16], [2, 3]]\n",
      " |      tf.pow(x, y) ==> [[256, 65536], [9, 27]]\n",
      " |      ```\n",
      " |      \n",
      " |      Args:\n",
      " |        x: A `Tensor` of type `float32`, `float64`, `int32`, `int64`, `complex64`,\n",
      " |         or `complex128`.\n",
      " |        y: A `Tensor` of type `float32`, `float64`, `int32`, `int64`, `complex64`,\n",
      " |         or `complex128`.\n",
      " |        name: A name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor`.\n",
      " |  \n",
      " |  __radd__ = _run_op(a, *args)\n",
      " |      Returns x + y element-wise.\n",
      " |      \n",
      " |      *NOTE*: `Add` supports broadcasting. `AddN` does not. More about broadcasting\n",
      " |      [here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)\n",
      " |      \n",
      " |      Args:\n",
      " |        x: A `Tensor`. Must be one of the following types: `half`, `float32`, `float64`, `uint8`, `int8`, `int16`, `int32`, `int64`, `complex64`, `complex128`, `string`.\n",
      " |        y: A `Tensor`. Must have the same type as `x`.\n",
      " |        name: A name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor`. Has the same type as `x`.\n",
      " |  \n",
      " |  __rand__ = _run_op(a, *args)\n",
      " |      Returns the truth value of x AND y element-wise.\n",
      " |      \n",
      " |      *NOTE*: `LogicalAnd` supports broadcasting. More about broadcasting\n",
      " |      [here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)\n",
      " |      \n",
      " |      Args:\n",
      " |        x: A `Tensor` of type `bool`.\n",
      " |        y: A `Tensor` of type `bool`.\n",
      " |        name: A name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor` of type `bool`.\n",
      " |  \n",
      " |  __rdiv__ = _run_op(a, *args)\n",
      " |      Divide two values using Python 2 semantics. Used for Tensor.__div__.\n",
      " |      \n",
      " |      Args:\n",
      " |        x: `Tensor` numerator of real numeric type.\n",
      " |        y: `Tensor` denominator of real numeric type.\n",
      " |        name: A name for the operation (optional).\n",
      " |      Returns:\n",
      " |        `x / y` returns the quotient of x and y.\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |  \n",
      " |  __rfloordiv__ = _run_op(a, *args)\n",
      " |      Divides `x / y` elementwise, rounding toward the most negative integer.\n",
      " |      \n",
      " |      The same as `tf.div(x,y)` for integers, but uses `tf.floor(tf.div(x,y))` for\n",
      " |      floating point arguments so that the result is always an integer (though\n",
      " |      possibly an integer represented as floating point).  This op is generated by\n",
      " |      `x // y` floor division in Python 3 and in Python 2.7 with\n",
      " |      `from __future__ import division`.\n",
      " |      \n",
      " |      Note that for efficiency, `floordiv` uses C semantics for negative numbers\n",
      " |      (unlike Python and Numpy).\n",
      " |      \n",
      " |      `x` and `y` must have the same type, and the result will have the same type\n",
      " |      as well.\n",
      " |      \n",
      " |      Args:\n",
      " |        x: `Tensor` numerator of real numeric type.\n",
      " |        y: `Tensor` denominator of real numeric type.\n",
      " |        name: A name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        `x / y` rounded down (except possibly towards zero for negative integers).\n",
      " |      \n",
      " |      Raises:\n",
      " |        TypeError: If the inputs are complex.\n",
      " |  \n",
      " |  __rmatmul__ = _run_op(a, *args)\n",
      " |      Multiplies matrix `a` by matrix `b`, producing `a` * `b`.\n",
      " |      \n",
      " |      The inputs must be matrices (or tensors of rank > 2, representing batches of\n",
      " |      matrices), with matching inner dimensions, possibly after transposition.\n",
      " |      \n",
      " |      Both matrices must be of the same type. The supported types are:\n",
      " |      `float16`, `float32`, `float64`, `int32`, `complex64`, `complex128`.\n",
      " |      \n",
      " |      Either matrix can be transposed or adjointed (conjugated and transposed) on\n",
      " |      the fly by setting one of the corresponding flag to `True`. These are `False`\n",
      " |      by default.\n",
      " |      \n",
      " |      If one or both of the matrices contain a lot of zeros, a more efficient\n",
      " |      multiplication algorithm can be used by setting the corresponding\n",
      " |      `a_is_sparse` or `b_is_sparse` flag to `True`. These are `False` by default.\n",
      " |      This optimization is only available for plain matrices (rank-2 tensors) with\n",
      " |      datatypes `bfloat16` or `float32`.\n",
      " |      \n",
      " |      For example:\n",
      " |      \n",
      " |      ```python\n",
      " |      # 2-D tensor `a`\n",
      " |      a = tf.constant([1, 2, 3, 4, 5, 6], shape=[2, 3]) => [[1. 2. 3.]\n",
      " |                                                            [4. 5. 6.]]\n",
      " |      # 2-D tensor `b`\n",
      " |      b = tf.constant([7, 8, 9, 10, 11, 12], shape=[3, 2]) => [[7. 8.]\n",
      " |                                                               [9. 10.]\n",
      " |                                                               [11. 12.]]\n",
      " |      c = tf.matmul(a, b) => [[58 64]\n",
      " |                              [139 154]]\n",
      " |      \n",
      " |      \n",
      " |      # 3-D tensor `a`\n",
      " |      a = tf.constant(np.arange(1, 13, dtype=np.int32),\n",
      " |                      shape=[2, 2, 3])                  => [[[ 1.  2.  3.]\n",
      " |                                                             [ 4.  5.  6.]],\n",
      " |                                                            [[ 7.  8.  9.]\n",
      " |                                                             [10. 11. 12.]]]\n",
      " |      \n",
      " |      # 3-D tensor `b`\n",
      " |      b = tf.constant(np.arange(13, 25, dtype=np.int32),\n",
      " |                      shape=[2, 3, 2])                   => [[[13. 14.]\n",
      " |                                                              [15. 16.]\n",
      " |                                                              [17. 18.]],\n",
      " |                                                             [[19. 20.]\n",
      " |                                                              [21. 22.]\n",
      " |                                                              [23. 24.]]]\n",
      " |      c = tf.matmul(a, b) => [[[ 94 100]\n",
      " |                               [229 244]],\n",
      " |                              [[508 532]\n",
      " |                               [697 730]]]\n",
      " |      \n",
      " |      # Since python >= 3.5 the @ operator is supported (see PEP 465).\n",
      " |      # In TensorFlow, it simply calls the `tf.matmul()` function, so the\n",
      " |      # following lines are equivalent:\n",
      " |      d = a @ b @ [[10.], [11.]]\n",
      " |      d = tf.matmul(tf.matmul(a, b), [[10.], [11.]])\n",
      " |      ```\n",
      " |      \n",
      " |      Args:\n",
      " |        a: `Tensor` of type `float16`, `float32`, `float64`, `int32`, `complex64`,\n",
      " |          `complex128` and rank > 1.\n",
      " |        b: `Tensor` with same type and rank as `a`.\n",
      " |        transpose_a: If `True`, `a` is transposed before multiplication.\n",
      " |        transpose_b: If `True`, `b` is transposed before multiplication.\n",
      " |        adjoint_a: If `True`, `a` is conjugated and transposed before\n",
      " |          multiplication.\n",
      " |        adjoint_b: If `True`, `b` is conjugated and transposed before\n",
      " |          multiplication.\n",
      " |        a_is_sparse: If `True`, `a` is treated as a sparse matrix.\n",
      " |        b_is_sparse: If `True`, `b` is treated as a sparse matrix.\n",
      " |        name: Name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor` of the same type as `a` and `b` where each inner-most matrix is\n",
      " |        the product of the corresponding matrices in `a` and `b`, e.g. if all\n",
      " |        transpose or adjoint attributes are `False`:\n",
      " |      \n",
      " |        `output`[..., i, j] = sum_k (`a`[..., i, k] * `b`[..., k, j]),\n",
      " |        for all indices i, j.\n",
      " |      \n",
      " |        Note: This is matrix product, not element-wise product.\n",
      " |      \n",
      " |      \n",
      " |      Raises:\n",
      " |        ValueError: If transpose_a and adjoint_a, or transpose_b and adjoint_b\n",
      " |          are both set to True.\n",
      " |  \n",
      " |  __rmod__ = _run_op(a, *args)\n",
      " |      Returns element-wise remainder of division. When `x < 0` xor `y < 0` is\n",
      " |      \n",
      " |      true, this follows Python semantics in that the result here is consistent\n",
      " |      with a flooring divide. E.g. `floor(x / y) * y + mod(x, y) = x`.\n",
      " |      \n",
      " |      *NOTE*: `FloorMod` supports broadcasting. More about broadcasting\n",
      " |      [here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)\n",
      " |      \n",
      " |      Args:\n",
      " |        x: A `Tensor`. Must be one of the following types: `int32`, `int64`, `float32`, `float64`.\n",
      " |        y: A `Tensor`. Must have the same type as `x`.\n",
      " |        name: A name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor`. Has the same type as `x`.\n",
      " |  \n",
      " |  __rmul__ = _run_op(a, *args)\n",
      " |      Dispatches cwise mul for \"Dense*Dense\" and \"Dense*Sparse\".\n",
      " |  \n",
      " |  __ror__ = _run_op(a, *args)\n",
      " |      Returns the truth value of x OR y element-wise.\n",
      " |      \n",
      " |      *NOTE*: `LogicalOr` supports broadcasting. More about broadcasting\n",
      " |      [here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)\n",
      " |      \n",
      " |      Args:\n",
      " |        x: A `Tensor` of type `bool`.\n",
      " |        y: A `Tensor` of type `bool`.\n",
      " |        name: A name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor` of type `bool`.\n",
      " |  \n",
      " |  __rpow__ = _run_op(a, *args)\n",
      " |      Computes the power of one value to another.\n",
      " |      \n",
      " |      Given a tensor `x` and a tensor `y`, this operation computes \\\\\\\\(x^y\\\\\\\\) for\n",
      " |      corresponding elements in `x` and `y`. For example:\n",
      " |      \n",
      " |      ```\n",
      " |      # tensor 'x' is [[2, 2], [3, 3]]\n",
      " |      # tensor 'y' is [[8, 16], [2, 3]]\n",
      " |      tf.pow(x, y) ==> [[256, 65536], [9, 27]]\n",
      " |      ```\n",
      " |      \n",
      " |      Args:\n",
      " |        x: A `Tensor` of type `float32`, `float64`, `int32`, `int64`, `complex64`,\n",
      " |         or `complex128`.\n",
      " |        y: A `Tensor` of type `float32`, `float64`, `int32`, `int64`, `complex64`,\n",
      " |         or `complex128`.\n",
      " |        name: A name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor`.\n",
      " |  \n",
      " |  __rsub__ = _run_op(a, *args)\n",
      " |      Returns x - y element-wise.\n",
      " |      \n",
      " |      *NOTE*: `Sub` supports broadcasting. More about broadcasting\n",
      " |      [here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)\n",
      " |      \n",
      " |      Args:\n",
      " |        x: A `Tensor`. Must be one of the following types: `half`, `float32`, `float64`, `int32`, `int64`, `complex64`, `complex128`.\n",
      " |        y: A `Tensor`. Must have the same type as `x`.\n",
      " |        name: A name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor`. Has the same type as `x`.\n",
      " |  \n",
      " |  __rtruediv__ = _run_op(a, *args)\n",
      " |  \n",
      " |  __rxor__ = _run_op(a, *args)\n",
      " |      x ^ y = (x | y) & ~(x & y).\n",
      " |  \n",
      " |  __sub__ = _run_op(a, *args)\n",
      " |      Returns x - y element-wise.\n",
      " |      \n",
      " |      *NOTE*: `Sub` supports broadcasting. More about broadcasting\n",
      " |      [here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)\n",
      " |      \n",
      " |      Args:\n",
      " |        x: A `Tensor`. Must be one of the following types: `half`, `float32`, `float64`, `int32`, `int64`, `complex64`, `complex128`.\n",
      " |        y: A `Tensor`. Must have the same type as `x`.\n",
      " |        name: A name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor`. Has the same type as `x`.\n",
      " |  \n",
      " |  __truediv__ = _run_op(a, *args)\n",
      " |  \n",
      " |  __xor__ = _run_op(a, *args)\n",
      " |      x ^ y = (x | y) & ~(x & y).\n",
      " |  \n",
      " |  assign(self, value, use_locking=False)\n",
      " |      Assigns a new value to the variable.\n",
      " |      \n",
      " |      This is essentially a shortcut for `assign(self, value)`.\n",
      " |      \n",
      " |      Args:\n",
      " |        value: A `Tensor`. The new value for this variable.\n",
      " |        use_locking: If `True`, use locking during the assignment.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor` that will hold the new value of this variable after\n",
      " |        the assignment has completed.\n",
      " |  \n",
      " |  assign_add(self, delta, use_locking=False)\n",
      " |      Adds a value to this variable.\n",
      " |      \n",
      " |       This is essentially a shortcut for `assign_add(self, delta)`.\n",
      " |      \n",
      " |      Args:\n",
      " |        delta: A `Tensor`. The value to add to this variable.\n",
      " |        use_locking: If `True`, use locking during the operation.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor` that will hold the new value of this variable after\n",
      " |        the addition has completed.\n",
      " |  \n",
      " |  assign_sub(self, delta, use_locking=False)\n",
      " |      Subtracts a value from this variable.\n",
      " |      \n",
      " |      This is essentially a shortcut for `assign_sub(self, delta)`.\n",
      " |      \n",
      " |      Args:\n",
      " |        delta: A `Tensor`. The value to subtract from this variable.\n",
      " |        use_locking: If `True`, use locking during the operation.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor` that will hold the new value of this variable after\n",
      " |        the subtraction has completed.\n",
      " |  \n",
      " |  count_up_to(self, limit)\n",
      " |      Increments this variable until it reaches `limit`.\n",
      " |      \n",
      " |      When that Op is run it tries to increment the variable by `1`. If\n",
      " |      incrementing the variable would bring it above `limit` then the Op raises\n",
      " |      the exception `OutOfRangeError`.\n",
      " |      \n",
      " |      If no error is raised, the Op outputs the value of the variable before\n",
      " |      the increment.\n",
      " |      \n",
      " |      This is essentially a shortcut for `count_up_to(self, limit)`.\n",
      " |      \n",
      " |      Args:\n",
      " |        limit: value at which incrementing the variable raises an error.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor` that will hold the variable value before the increment. If no\n",
      " |        other Op modifies this variable, the values produced will all be\n",
      " |        distinct.\n",
      " |  \n",
      " |  eval(self, session=None)\n",
      " |      In a session, computes and returns the value of this variable.\n",
      " |      \n",
      " |      This is not a graph construction method, it does not add ops to the graph.\n",
      " |      \n",
      " |      This convenience method requires a session where the graph\n",
      " |      containing this variable has been launched. If no session is\n",
      " |      passed, the default session is used.  See @{tf.Session} for more\n",
      " |      information on launching a graph and on sessions.\n",
      " |      \n",
      " |      ```python\n",
      " |      v = tf.Variable([1, 2])\n",
      " |      init = tf.global_variables_initializer()\n",
      " |      \n",
      " |      with tf.Session() as sess:\n",
      " |          sess.run(init)\n",
      " |          # Usage passing the session explicitly.\n",
      " |          print(v.eval(sess))\n",
      " |          # Usage with the default session.  The 'with' block\n",
      " |          # above makes 'sess' the default session.\n",
      " |          print(v.eval())\n",
      " |      ```\n",
      " |      \n",
      " |      Args:\n",
      " |        session: The session to use to evaluate this variable. If\n",
      " |          none, the default session is used.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A numpy `ndarray` with a copy of the value of this variable.\n",
      " |  \n",
      " |  get_shape(self)\n",
      " |      Alias of Variable.shape.\n",
      " |  \n",
      " |  initialized_value(self)\n",
      " |      Returns the value of the initialized variable.\n",
      " |      \n",
      " |      You should use this instead of the variable itself to initialize another\n",
      " |      variable with a value that depends on the value of this variable.\n",
      " |      \n",
      " |      ```python\n",
      " |      # Initialize 'v' with a random tensor.\n",
      " |      v = tf.Variable(tf.truncated_normal([10, 40]))\n",
      " |      # Use `initialized_value` to guarantee that `v` has been\n",
      " |      # initialized before its value is used to initialize `w`.\n",
      " |      # The random values are picked only once.\n",
      " |      w = tf.Variable(v.initialized_value() * 2.0)\n",
      " |      ```\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor` holding the value of this variable after its initializer\n",
      " |        has run.\n",
      " |  \n",
      " |  load(self, value, session=None)\n",
      " |      Load new value into this variable\n",
      " |      \n",
      " |      Writes new value to variable's memory. Doesn't add ops to the graph.\n",
      " |      \n",
      " |      This convenience method requires a session where the graph\n",
      " |      containing this variable has been launched. If no session is\n",
      " |      passed, the default session is used.  See @{tf.Session} for more\n",
      " |      information on launching a graph and on sessions.\n",
      " |      \n",
      " |      ```python\n",
      " |      v = tf.Variable([1, 2])\n",
      " |      init = tf.global_variables_initializer()\n",
      " |      \n",
      " |      with tf.Session() as sess:\n",
      " |          sess.run(init)\n",
      " |          # Usage passing the session explicitly.\n",
      " |          v.load([2, 3], sess)\n",
      " |          print(v.eval(sess)) # prints [2 3]\n",
      " |          # Usage with the default session.  The 'with' block\n",
      " |          # above makes 'sess' the default session.\n",
      " |          v.load([3, 4], sess)\n",
      " |          print(v.eval()) # prints [3 4]\n",
      " |      ```\n",
      " |      \n",
      " |      Args:\n",
      " |          value: New variable value\n",
      " |          session: The session to use to evaluate this variable. If\n",
      " |            none, the default session is used.\n",
      " |      \n",
      " |      Raises:\n",
      " |          ValueError: Session is not passed and no default session\n",
      " |  \n",
      " |  read_value(self)\n",
      " |      Returns the value of this variable, read in the current context.\n",
      " |      \n",
      " |      Can be different from value() if it's on another device, with control\n",
      " |      dependencies, etc.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor` containing the value of the variable.\n",
      " |  \n",
      " |  scatter_sub(self, sparse_delta, use_locking=False)\n",
      " |      Subtracts `IndexedSlices` from this variable.\n",
      " |      \n",
      " |      This is essentially a shortcut for `scatter_sub(self, sparse_delta.indices,\n",
      " |      sparse_delta.values)`.\n",
      " |      \n",
      " |      Args:\n",
      " |        sparse_delta: `IndexedSlices` to be subtracted from this variable.\n",
      " |        use_locking: If `True`, use locking during the operation.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor` that will hold the new value of this variable after\n",
      " |        the scattered subtraction has completed.\n",
      " |      \n",
      " |      Raises:\n",
      " |        ValueError: if `sparse_delta` is not an `IndexedSlices`.\n",
      " |  \n",
      " |  set_shape(self, shape)\n",
      " |      Overrides the shape for this variable.\n",
      " |      \n",
      " |      Args:\n",
      " |        shape: the `TensorShape` representing the overridden shape.\n",
      " |  \n",
      " |  to_proto(self, export_scope=None)\n",
      " |      Converts a `Variable` to a `VariableDef` protocol buffer.\n",
      " |      \n",
      " |      Args:\n",
      " |        export_scope: Optional `string`. Name scope to remove.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `VariableDef` protocol buffer, or `None` if the `Variable` is not\n",
      " |        in the specified name scope.\n",
      " |  \n",
      " |  value(self)\n",
      " |      Returns the last snapshot of this variable.\n",
      " |      \n",
      " |      You usually do not need to call this method as all ops that need the value\n",
      " |      of the variable call it automatically through a `convert_to_tensor()` call.\n",
      " |      \n",
      " |      Returns a `Tensor` which holds the value of the variable.  You can not\n",
      " |      assign a new value to this tensor as it is not a reference to the variable.\n",
      " |      \n",
      " |      To avoid copies, if the consumer of the returned value is on the same device\n",
      " |      as the variable, this actually returns the live value of the variable, not\n",
      " |      a copy.  Updates to the variable are seen by the consumer.  If the consumer\n",
      " |      is on a different device it will get a copy of the variable.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor` containing the value of the variable.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods defined here:\n",
      " |  \n",
      " |  from_proto(variable_def, import_scope=None)\n",
      " |      Returns a `Variable` object created from `variable_def`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  device\n",
      " |      The device of this variable.\n",
      " |  \n",
      " |  dtype\n",
      " |      The `DType` of this variable.\n",
      " |  \n",
      " |  graph\n",
      " |      The `Graph` of this variable.\n",
      " |  \n",
      " |  initial_value\n",
      " |      Returns the Tensor used as the initial value for the variable.\n",
      " |      \n",
      " |      Note that this is different from `initialized_value()` which runs\n",
      " |      the op that initializes the variable before returning its value.\n",
      " |      This method returns the tensor that is used by the op that initializes\n",
      " |      the variable.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor`.\n",
      " |  \n",
      " |  initializer\n",
      " |      The initializer operation for this variable.\n",
      " |  \n",
      " |  name\n",
      " |      The name of this variable.\n",
      " |  \n",
      " |  op\n",
      " |      The `Operation` of this variable.\n",
      " |  \n",
      " |  shape\n",
      " |      The `TensorShape` of this variable.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `TensorShape`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  SaveSliceInfo = <class 'tensorflow.python.ops.variables.SaveSliceInfo'...\n",
      " |      Information on how to save this Variable as a slice.\n",
      " |      \n",
      " |      Provides internal support for saving variables as slices of a larger\n",
      " |      variable.  This API is not public and is subject to change.\n",
      " |      \n",
      " |      Available properties:\n",
      " |      \n",
      " |      * full_name\n",
      " |      * full_shape\n",
      " |      * var_offset\n",
      " |      * var_shape\n",
      " |  \n",
      " |  __array_priority__ = 100\n",
      "\n"
     ]
    }
   ],
   "source": [
    "state = tf.Variable(10,name = \"counter\")\n",
    "help(tf.Variable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tensorflow.Variable(value,name = \"xxx\")设置一个变量，value 是该变量的初始值。并且如果需要使用变量时，必须先使用init函数初始化变量的值;name 用于设置变量的名称给Operation使用 Q？name 与变量赋值的量有啥区别（例如上文的state 和counter）\n",
    "- `tensorflow.global_variables_initializer()` 用于全局变量初始化操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "one = tf.constant(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- tensorflow.constant(value) : 用于设置一个常量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Add:0\", shape=(), dtype=int32)\n",
      "Tensor(\"Assign:0\", shape=(), dtype=int32_ref)\n",
      "Help on function assign in module tensorflow.python.ops.state_ops:\n",
      "\n",
      "assign(ref, value, validate_shape=None, use_locking=None, name=None)\n",
      "    Update 'ref' by assigning 'value' to it.\n",
      "    \n",
      "    This operation outputs a Tensor that holds the new value of 'ref' after\n",
      "      the value has been assigned. This makes it easier to chain operations\n",
      "      that need to use the reset value.\n",
      "    \n",
      "    Args:\n",
      "      ref: A mutable `Tensor`.\n",
      "        Should be from a `Variable` node. May be uninitialized.\n",
      "      value: A `Tensor`. Must have the same type as `ref`.\n",
      "        The value to be assigned to the variable.\n",
      "      validate_shape: An optional `bool`. Defaults to `True`.\n",
      "        If true, the operation will validate that the shape\n",
      "        of 'value' matches the shape of the Tensor being assigned to.  If false,\n",
      "        'ref' will take on the shape of 'value'.\n",
      "      use_locking: An optional `bool`. Defaults to `True`.\n",
      "        If True, the assignment will be protected by a lock;\n",
      "        otherwise the behavior is undefined, but may exhibit less contention.\n",
      "      name: A name for the operation (optional).\n",
      "    \n",
      "    Returns:\n",
      "      A `Tensor` that will hold the new value of 'ref' after\n",
      "        the assignment has completed.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_value = tf.add(state,one)\n",
    "print new_value\n",
    "update = tf.assign(state,new_value)\n",
    "print update\n",
    "help(tf.assign)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tensor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "print the init value: 10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "2.now the value of state is: 10\n",
      "3.now the value of state is: 11\n",
      "4.now the value of state is: 10\n"
     ]
    }
   ],
   "source": [
    "init_op = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init_op)\n",
    "    print \"print the init value:\",sess.run(state)\n",
    "    for _ in range(10):\n",
    "        sess.run(update)\n",
    "        print sess.run(state)\n",
    "my_sess = tf.Session()\n",
    "# the next code will show an error:FailedPreconditionError: Attempting to use uninitialized value counter_5\n",
    "# print \"1.now the value of state is:\",my_sess.run(state)\n",
    "my_sess.run(init_op)\n",
    "print \"2.now the value of state is:\",my_sess.run(state)\n",
    "my_sess.run(update)\n",
    "print \"3.now the value of state is:\",my_sess.run(state)\n",
    "my_sess.run(init_op)\n",
    "print \"4.now the value of state is:\",my_sess.run(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Placeholder_21:0\", dtype=float32)\n",
      "[array([ 14.], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "input1 = tf.placeholder(tf.float32)\n",
    "input2 = tf.placeholder(tf.float32)\n",
    "output = tf.multiply(input1,input2)\n",
    "print input1 \n",
    "with tf.Session() as sess:\n",
    "    print sess.run([output],feed_dict = {input1:[7.],input2:[2.]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- tensorflow.placeholder()设置一个占位符。"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST\n",
    "## 数据集获得\n",
    "数据集分为俩部分 训练集和测试集。\n",
    "图片数据抽象为28x28的数组，然后展开为784的向量，向量每个元素代表像素的强弱-值为[0,1]\n",
    "\n",
    "标签数据为0-9 九个数字，使用one-hot vecters 展开为10维向量[1,0,0,0,0,0,0,0,0,0]其中只有一个元素为1，其余元素为0\n",
    "\n",
    "\n",
    "- Softmax回归\n",
    "给定一个图片后，需要给出对应table的概率\n",
    "Screen Shot 2017-07-14 at 10.59.58 AM![image.png](attachment:image.png)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
